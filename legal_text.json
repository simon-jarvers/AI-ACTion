{
  "Article 9": {
    "title": "Risk management system",
    "articles": {
      "1": {
        "content": "A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems.",
        "articles": {}
      },
      "2": {
        "content": "The risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic review and updating. It shall comprise the following steps:",
        "articles": {
          "a": {
            "content": "the identification and analysis of the known and the reasonably foreseeable risks that the high-risk AI system can pose to health, safety or fundamental rights when the high-risk AI system is used in accordance with its intended purpose;",
            "articles": {}
          },
          "b": {
            "content": "the estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose, and under conditions of reasonably foreseeable misuse;",
            "articles": {}
          },
          "c": {
            "content": "the evaluation of other risks possibly arising, based on the analysis of data gathered from the post-market monitoring system referred to in Article 72;",
            "articles": {}
          },
          "d": {
            "content": "the adoption of appropriate and targeted risk management measures designed to address the risks identified pursuant to point (a).",
            "articles": {}
          }
        }
      },
      "3": {
        "content": "The risks referred to in this Article shall concern only those which may be reasonably mitigated or eliminated through the development or design of the high-risk AI system, or the provision of adequate technical information.",
        "articles": {}
      },
      "4": {
        "content": "The risk management measures referred to in paragraph 2, point (d), shall give due consideration to the effects and possible interaction resulting from the combined application of the requirements set out in this Section, with a view to minimising risks more effectively while achieving an appropriate balance in implementing the measures to fulfil those requirements.",
        "articles": {}
      },
      "5": {
        "content": "The risk management measures referred to in paragraph 2, point (d), shall be such that the relevant residual risk associated with each hazard, as well as the overall residual risk of the high-risk AI systems is judged to be acceptable.",
        "articles": {
          "a": {
            "content": "In identifying the most appropriate risk management measures, the following shall be ensured:",
            "articles": {
              "i": {
                "content": "elimination or reduction of risks identified and evaluated pursuant to paragraph 2 in as far as technically feasible through adequate design and development of the high-risk AI system;",
                "articles": {}
              },
              "ii": {
                "content": "where appropriate, implementation of adequate mitigation and control measures addressing risks that cannot be eliminated;",
                "articles": {}
              },
              "iii": {
                "content": "provision of information required pursuant to Article 13 and, where appropriate, training to deployers.",
                "articles": {}
              }
            }
          },
          "b": {
            "content": "With a view to eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, the training to be expected by the deployer, and the presumable context in which the system is intended to be used.",
            "articles": {}
          }
        }
      },
      "6": {
        "content": "High-risk AI systems shall be tested for the purpose of identifying the most appropriate and targeted risk management measures. Testing shall ensure that high-risk AI systems perform consistently for their intended purpose and that they are in compliance with the requirements set out in this Section.",
        "articles": {}
      },
      "7": {
        "content": "Testing procedures may include testing in real-world conditions in accordance with Article 60.",
        "articles": {}
      },
      "8": {
        "content": "The testing of high-risk AI systems shall be performed, as appropriate, at any time throughout the development process, and, in any event, prior to their being placed on the market or put into service. Testing shall be carried out against prior defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system.",
        "articles": {}
      },
      "9": {
        "content": "When implementing the risk management system as provided for in paragraphs 1 to 7, providers shall give consideration to whether in view of its intended purpose the high-risk AI system is likely to have an adverse impact on persons under the age of 18 and, as appropriate, other vulnerable groups.",
        "articles": {}
      },
      "10": {
        "content": "For providers of high-risk AI systems that are subject to requirements regarding internal risk management processes under other relevant provisions of Union law, the aspects provided in paragraphs 1 to 9 may be part of, or combined with, the risk management procedures established pursuant to that law.",
        "articles": {}
      }
    }
  },
  "Article 10": {
    "title": "Data and data governance",
    "articles": {
      "1": {
        "content": "High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used.",
        "articles": {}
      },
      "2": {
        "content": "Training, validation and testing data sets shall be subject to data governance and management practices appropriate for the intended purpose of the high-risk AI system. Those practices shall concern in particular:",
        "articles": {
          "a": {
            "content": "the relevant design choices;",
            "articles": {}
          },
          "b": {
            "content": "data collection processes and the origin of data, and in the case of personal data, the original purpose of the data collection;",
            "articles": {}
          },
          "c": {
            "content": "relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and aggregation;",
            "articles": {}
          },
          "d": {
            "content": "the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and represent;",
            "articles": {}
          },
          "e": {
            "content": "an assessment of the availability, quantity and suitability of the data sets that are needed;",
            "articles": {}
          },
          "f": {
            "content": "examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations;",
            "articles": {}
          },
          "g": {
            "content": "appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);",
            "articles": {}
          },
          "h": {
            "content": "the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those gaps and shortcomings can be addressed.",
            "articles": {}
          }
        }
      },
      "3": {
        "content": "Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used. Those characteristics of the data sets may be met at the level of individual data sets or at the level of a combination thereof.",
        "articles": {}
      },
      "4": {
        "content": "Data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, contextual, behavioural or functional setting within which the high-risk AI system is intended to be used.",
        "articles": {}
      },
      "5": {
        "content": "To the extent that it is strictly necessary for the purpose of ensuring bias detection and correction in relation to the high-risk AI systems in accordance with paragraph (2), points (f) and (g) of this Article, the providers of such systems may exceptionally process special categories of personal data, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons. In addition to the provisions set out in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, all the following conditions must be met in order for such processing to occur:",
        "articles": {
          "a": {
            "content": "the bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic or anonymised data;",
            "articles": {}
          },
          "b": {
            "content": "the special categories of personal data are subject to technical limitations on the re-use of the personal data, and state-of-the-art security and privacy-preserving measures, including pseudonymisation;",
            "articles": {}
          },
          "c": {
            "content": "the special categories of personal data are subject to measures to ensure that the personal data processed are secured, protected, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse and ensure that only authorised persons have access to those personal data with appropriate confidentiality obligations;",
            "articles": {}
          },
          "d": {
            "content": "the special categories of personal data are not to be transmitted, transferred or otherwise accessed by other parties;",
            "articles": {}
          },
          "e": {
            "content": "the special categories of personal data are deleted once the bias has been corrected or the personal data has reached the end of its retention period, whichever comes first;",
            "articles": {}
          },
          "f": {
            "content": "the records of processing activities pursuant to Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 include the reasons why the processing of special categories of personal data was strictly necessary to detect and correct biases, and why that objective could not be achieved by processing other data.",
            "articles": {}
          }
        }
      },
      "6": {
        "content": "For the development of high-risk AI systems not using techniques involving the training of AI models, paragraphs 2 to 5 apply only to the testing data sets.",
        "articles": {}
      }
    }
  },
  "Article 12": {
    "title": "Record-keeping",
    "articles": {
      "1": {
        "content": "High-risk AI systems shall technically allow for the automatic recording of events (logs) over the lifetime of the system.",
        "articles": {}
      },
      "2": {
        "content": "In order to ensure a level of traceability of the functioning of a high-risk AI system that is appropriate to the intended purpose of the system, logging capabilities shall enable the recording of events relevant for:",
        "articles": {
          "a": {
            "content": "identifying situations that may result in the high-risk AI system presenting a risk within the meaning of Article 79(1) or in a substantial modification;",
            "articles": {}
          },
          "b": {
            "content": "facilitating the post-market monitoring referred to in Article 72; and",
            "articles": {}
          },
          "c": {
            "content": "monitoring the operation of high-risk AI systems referred to in Article 26(5).",
            "articles": {}
          }
        }
      },
      "3": {
        "content": "For high-risk AI systems referred to in point 1 (a), of Annex III, the logging capabilities shall provide, at a minimum:",
        "articles": {
          "a": {
            "content": "recording of the period of each use of the system (start date and time and end date and time of each use);",
            "articles": {}
          },
          "b": {
            "content": "the reference database against which input data has been checked by the system;",
            "articles": {}
          },
          "c": {
            "content": "the input data for which the search has led to a match;",
            "articles": {}
          },
          "d": {
            "content": "the identification of the natural persons involved in the verification of the results, as referred to in Article 14(5).",
            "articles": {}
          }
        }
      }
    }
  },
  "Article 13": {
    "title": "Transparency and provision of information to deployers",
    "articles": {
      "1": {
        "content": "High-risk AI systems shall be designed and developed in such a way as to ensure that their operation is sufficiently transparent to enable deployers to interpret a system's output and use it appropriately. An appropriate type and degree of transparency shall be ensured with a view to achieving compliance with the relevant obligations of the provider and deployer set out in Section 3.",
        "articles": {}
      },
      "2": {
        "content": "High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to deployers.",
        "articles": {}
      },
      "3": {
        "content": "The instructions for use shall contain at least the following information:",
        "articles": {
          "a": {
            "content": "the identity and the contact details of the provider and, where applicable, of its authorised representative;",
            "articles": {}
          },
          "b": {
            "content": "the characteristics, capabilities and limitations of performance of the high-risk AI system, including:",
            "articles": {
              "i": {
                "content": "its intended purpose;",
                "articles": {}
              },
              "ii": {
                "content": "the level of accuracy, including its metrics, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity;",
                "articles": {}
              },
              "iii": {
                "content": "any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights referred to in Article 9(2);",
                "articles": {}
              },
              "iv": {
                "content": "where applicable, the technical capabilities and characteristics of the high-risk AI system to provide information that is relevant to explain its output;",
                "articles": {}
              },
              "v": {
                "content": "when appropriate, its performance regarding specific persons or groups of persons on which the system is intended to be used;",
                "articles": {}
              },
              "vi": {
                "content": "when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the high-risk AI system;",
                "articles": {}
              },
              "vii": {
                "content": "where applicable, information to enable deployers to interpret the output of the high-risk AI system and use it appropriately;",
                "articles": {}
              }
            }
          },
          "c": {
            "content": "the changes to the high-risk AI system and its performance which have been pre-determined by the provider at the moment of the initial conformity assessment, if any;",
            "articles": {}
          },
          "d": {
            "content": "the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of the high-risk AI systems by the deployers;",
            "articles": {}
          },
          "e": {
            "content": "the computational and hardware resources needed, the expected lifetime of the high-risk AI system and any necessary maintenance and care measures, including their frequency, to ensure the proper functioning of that AI system, including as regards software updates;",
            "articles": {}
          },
          "f": {
            "content": "where relevant, a description of the mechanisms included within the high-risk AI system that allows deployers to properly collect, store and interpret the logs in accordance with Article 12.",
            "articles": {}
          }
        }
      }
    }
  },
  "Article 14": {
    "title": "Human oversight",
    "articles": {
      "1": {
        "content": "High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which they are in use.",
        "articles": {}
      },
      "2": {
        "content": "Human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular where such risks persist despite the application of other requirements set out in this Section.",
        "articles": {}
      },
      "3": {
        "content": "The oversight measures shall be commensurate with the risks, level of autonomy and context of use of the high-risk AI system, and shall be ensured through either one or both of the following types of measures:",
        "articles": {
          "a": {
            "content": "measures identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the market or put into service;",
            "articles": {}
          },
          "b": {
            "content": "measures identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the deployer.",
            "articles": {}
          }
        }
      },
      "4": {
        "content": "For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be provided to the deployer in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate:",
        "articles": {
          "a": {
            "content": "to properly understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor its operation, including in view of detecting and addressing anomalies, dysfunctions and unexpected performance;",
            "articles": {}
          },
          "b": {
            "content": "to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons;",
            "articles": {}
          },
          "c": {
            "content": "to correctly interpret the high-risk AI system's output, taking into account, for example, the interpretation tools and methods available;",
            "articles": {}
          },
          "d": {
            "content": "to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reverse the output of the high-risk AI system;",
            "articles": {}
          },
          "e": {
            "content": "to intervene in the operation of the high-risk AI system or interrupt the system through a 'stop' button or a similar procedure that allows the system to come to a halt in a safe state.",
            "articles": {}
          }
        }
      },
      "5": {
        "content": "For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article shall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification resulting from the system unless that identification has been separately verified and confirmed by at least two natural persons with the necessary competence, training and authority. The requirement for a separate verification by at least two natural persons shall not apply to high-risk AI systems used for the purposes of law enforcement, migration, border control or asylum, where Union or national law considers the application of this requirement to be disproportionate.",
        "articles": {}
      }
    }
  },
  "Article 15": {
    "title": "Accuracy, robustness and cybersecurity",
    "articles": {
      "1": {
        "content": "High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and that they perform consistently in those respects throughout their lifecycle.",
        "articles": {}
      },
      "2": {
        "content": "To address the technical aspects of how to measure the appropriate levels of accuracy and robustness set out in paragraph 1 and any other relevant performance metrics, the Commission shall, in cooperation with relevant stakeholders and organisations such as metrology and benchmarking authorities, encourage, as appropriate, the development of benchmarks and measurement methodologies.",
        "articles": {}
      },
      "3": {
        "content": "The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use.",
        "articles": {}
      },
      "4": {
        "content": "High-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems. Technical and organisational measures shall be taken in this regard. The robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans. High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way as to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (feedback loops), and as to ensure that any such feedback loops are duly addressed with appropriate mitigation measures.",
        "articles": {}
      },
      "5": {
        "content": "High-risk AI systems shall be resilient against attempts by unauthorised third parties to alter their use, outputs or performance by exploiting system vulnerabilities. The technical solutions aiming to ensure the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks. The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying to manipulate the training data set (data poisoning), or pre-trained components used in training (model poisoning), inputs designed to cause the AI model to make a mistake (adversarial examples or model evasion), confidentiality attacks or model flaws.",
        "articles": {}
      }
    }
  },
  "Article 72": {
    "title": "Post-market monitoring by providers and post-market monitoring plan for high-risk AI systems",
    "articles": {
      "1": {
        "content": "Providers shall establish and document a post-market monitoring system in a manner that is proportionate to the nature of the AI technologies and the risks of the high-risk AI system.",
        "articles": {}
      },
      "2": {
        "content": "The post-market monitoring system shall actively and systematically collect, document and analyse relevant data which may be provided by deployers or which may be collected through other sources on the performance of high-risk AI systems throughout their lifetime, and which allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Chapter III, Section 2. Where relevant, post-market monitoring shall include an analysis of the interaction with other AI systems. This obligation shall not cover sensitive operational data of deployers which are law-enforcement authorities.",
        "articles": {}
      },
      "3": {
        "content": "The post-market monitoring system shall be based on a post-market monitoring plan. The post-market monitoring plan shall be part of the technical documentation referred to in Annex IV. The Commission shall adopt an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan and the list of elements to be included in the plan by 2 February 2026. That implementing act shall be adopted in accordance with the examination procedure referred to in Article 98(2).",
        "articles": {}
      },
      "4": {
        "content": "For high-risk AI systems covered by the Union harmonisation legislation listed in Section A of Annex I, where a post-market monitoring system and plan are already established under that legislation, in order to ensure consistency, avoid duplications and minimise additional burdens, providers shall have a choice of integrating, as appropriate, the necessary elements described in paragraphs 1, 2 and 3 using the template referred in paragraph 3 into systems and plans already existing under that legislation, provided that it achieves an equivalent level of protection. The first subparagraph of this paragraph shall also apply to high-risk AI systems referred to in point 5 of Annex III placed on the market or put into service by financial institutions that are subject to requirements under Union financial services law regarding their internal governance, arrangements or processes.",
        "articles": {}
      }
    }
  },
  "Annex IV": {
    "title": "Technical documentation referred to in Article 11(1)",
    "articles": {
      "intro": {
        "content": "The technical documentation referred to in Article 11(1) shall contain at least the following information, as applicable to the relevant AI system:",
        "articles": {}
      },
      "1": {
        "content": "A general description of the AI system including:",
        "articles": {
          "a": {
            "content": "its intended purpose, the name of the provider and the version of the system reflecting its relation to previous versions;",
            "articles": {}
          },
          "b": {
            "content": "how the AI system interacts with, or can be used to interact with, hardware or software, including with other AI systems, that are not part of the AI system itself, where applicable;",
            "articles": {}
          },
          "c": {
            "content": "the versions of relevant software or firmware, and any requirements related to version updates;",
            "articles": {}
          },
          "d": {
            "content": "the description of all the forms in which the AI system is placed on the market or put into service, such as software packages embedded into hardware, downloads, or APIs;",
            "articles": {}
          },
          "e": {
            "content": "the description of the hardware on which the AI system is intended to run;",
            "articles": {}
          },
          "f": {
            "content": "where the AI system is a component of products, photographs or illustrations showing external features, the marking and internal layout of those products;",
            "articles": {}
          },
          "g": {
            "content": "a basic description of the user-interface provided to the deployer;",
            "articles": {}
          },
          "h": {
            "content": "instructions for use for the deployer, and a basic description of the user-interface provided to the deployer, where applicable;",
            "articles": {}
          }
        }
      },
      "2": {
        "content": "A detailed description of the elements of the AI system and of the process for its development, including:",
        "articles": {
          "a": {
            "content": "the methods and steps performed for the development of the AI system, including, where relevant, recourse to pre-trained systems or tools provided by third parties and how those were used, integrated or modified by the provider;",
            "articles": {}
          },
          "b": {
            "content": "the design specifications of the system, namely the general logic of the AI system and of the algorithms; the key design choices including the rationale and assumptions made, including with regard to persons or groups of persons in respect of who, the system is intended to be used; the main classification choices; what the system is designed to optimise for, and the relevance of the different parameters; the description of the expected output and output quality of the system; the decisions about any possible trade-off made regarding the technical solutions adopted to comply with the requirements set out in Chapter III, Section 2;",
            "articles": {}
          },
          "c": {
            "content": "the description of the system architecture explaining how software components build on or feed into each other and integrate into the overall processing; the computational resources used to develop, train, test and validate the AI system;",
            "articles": {}
          },
          "d": {
            "content": "where relevant, the data requirements in terms of datasheets describing the training methodologies and techniques and the training data sets used, including a general description of these data sets, information about their provenance, scope and main characteristics; how the data was obtained and selected; labelling procedures (e.g. for supervised learning), data cleaning methodologies (e.g. outliers detection);",
            "articles": {}
          },
          "e": {
            "content": "assessment of the human oversight measures needed in accordance with Article 14, including an assessment of the technical measures needed to facilitate the interpretation of the outputs of AI systems by the deployers, in accordance with Article 13(3), point (d);",
            "articles": {}
          },
          "f": {
            "content": "where applicable, a detailed description of pre-determined changes to the AI system and its performance, together with all the relevant information related to the technical solutions adopted to ensure continuous compliance of the AI system with the relevant requirements set out in Chapter III, Section 2;",
            "articles": {}
          },
          "g": {
            "content": "the validation and testing procedures used, including information about the validation and testing data used and their main characteristics; metrics used to measure accuracy, robustness and compliance with other relevant requirements set out in Chapter III, Section 2, as well as potentially discriminatory impacts; test logs and all test reports dated and signed by the responsible persons, including with regard to pre-determined changes as referred to under point (f);",
            "articles": {}
          },
          "h": {
            "content": "cybersecurity measures put in place;",
            "articles": {}
          }
        }
      },
      "3": {
        "content": "Detailed information about the monitoring, functioning and control of the AI system, in particular with regard to: its capabilities and limitations in performance, including the degrees of accuracy for specific persons or groups of persons on which the system is intended to be used and the overall expected level of accuracy in relation to its intended purpose; the foreseeable unintended outcomes and sources of risks to health and safety, fundamental rights and discrimination in view of the intended purpose of the AI system; the human oversight measures needed in accordance with Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the deployers; specifications on input data, as appropriate;",
        "articles": {}
      },
      "4": {
        "content": "A description of the appropriateness of the performance metrics for the specific AI system;",
        "articles": {}
      },
      "5": {
        "content": "A detailed description of the risk management system in accordance with Article 9;",
        "articles": {}
      },
      "6": {
        "content": "A description of relevant changes made by the provider to the system through its lifecycle;",
        "articles": {}
      },
      "7": {
        "content": "A list of the harmonised standards applied in full or in part the references of which have been published in the Official Journal of the European Union; where no such harmonised standards have been applied, a detailed description of the solutions adopted to meet the requirements set out in Chapter III, Section 2, including a list of other relevant standards and technical specifications applied;",
        "articles": {}
      },
      "8": {
        "content": "A copy of the EU declaration of conformity referred to in Article 47;",
        "articles": {}
      },
      "9": {
        "content": "A detailed description of the system in place to evaluate the AI system performance in the post-market phase in accordance with Article 72, including the post-market monitoring plan referred to in Article 72(3).",
        "articles": {}
      }
    }
  }
}
